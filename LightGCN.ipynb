{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import math"
      ],
      "metadata": {
        "id": "wPbxJKZ-jNW6"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SEED = 2020\n",
        "\n",
        "def set_seeds(seed=SEED):\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "\n",
        "set_seeds()\n",
        "print(f\"Ziarno losowości ustawione na: {SEED}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pm2PEZY2JZ6p",
        "outputId": "66a081da-85ad-48e6-e812-4810265dc635"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ziarno losowości ustawione na: 2020\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p /content/data/amazon-book\n",
        "\n",
        "BASE=\"https://raw.githubusercontent.com/kuandeng/LightGCN/master/Data/amazon-book\"\n",
        "\n",
        "!wget $BASE/train.txt      -P /content/data/amazon-book\n",
        "!wget $BASE/test.txt       -P /content/data/amazon-book\n",
        "!wget $BASE/user_list.txt  -P /content/data/amazon-book\n",
        "!wget $BASE/item_list.txt  -P /content/data/amazon-book\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2KOlDyVTh2pD",
        "outputId": "78295c0b-377d-4b23-cf7f-0e193eb41034"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-11-29 15:07:09--  https://raw.githubusercontent.com/kuandeng/LightGCN/master/Data/amazon-book/train.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 14125691 (13M) [text/plain]\n",
            "Saving to: ‘/content/data/amazon-book/train.txt’\n",
            "\n",
            "train.txt           100%[===================>]  13.47M  --.-KB/s    in 0.08s   \n",
            "\n",
            "2025-11-29 15:07:09 (170 MB/s) - ‘/content/data/amazon-book/train.txt’ saved [14125691/14125691]\n",
            "\n",
            "--2025-11-29 15:07:10--  https://raw.githubusercontent.com/kuandeng/LightGCN/master/Data/amazon-book/test.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3848611 (3.7M) [text/plain]\n",
            "Saving to: ‘/content/data/amazon-book/test.txt’\n",
            "\n",
            "test.txt            100%[===================>]   3.67M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2025-11-29 15:07:10 (79.8 MB/s) - ‘/content/data/amazon-book/test.txt’ saved [3848611/3848611]\n",
            "\n",
            "--2025-11-29 15:07:10--  https://raw.githubusercontent.com/kuandeng/LightGCN/master/Data/amazon-book/user_list.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1080734 (1.0M) [text/plain]\n",
            "Saving to: ‘/content/data/amazon-book/user_list.txt’\n",
            "\n",
            "user_list.txt       100%[===================>]   1.03M  --.-KB/s    in 0.03s   \n",
            "\n",
            "2025-11-29 15:07:10 (32.1 MB/s) - ‘/content/data/amazon-book/user_list.txt’ saved [1080734/1080734]\n",
            "\n",
            "--2025-11-29 15:07:10--  https://raw.githubusercontent.com/kuandeng/LightGCN/master/Data/amazon-book/item_list.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1546089 (1.5M) [text/plain]\n",
            "Saving to: ‘/content/data/amazon-book/item_list.txt’\n",
            "\n",
            "item_list.txt       100%[===================>]   1.47M  --.-KB/s    in 0.04s   \n",
            "\n",
            "2025-11-29 15:07:10 (40.0 MB/s) - ‘/content/data/amazon-book/item_list.txt’ saved [1546089/1546089]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_amazon_book_train(data_dir):\n",
        "    \"\"\"\n",
        "    data_dir: katalog z plikiem train.txt\n",
        "    Zwraca:\n",
        "      num_users, num_items,\n",
        "      user_ids (np.array),\n",
        "      item_ids (np.array),\n",
        "      user_pos_items: dict u -> set(itemów)\n",
        "    \"\"\"\n",
        "    train_path = os.path.join(data_dir, \"train.txt\")\n",
        "    user_pos_items = {}\n",
        "    user_ids = []\n",
        "    item_ids = []\n",
        "\n",
        "    with open(train_path, \"r\") as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split()\n",
        "            if len(parts) <= 1:\n",
        "                continue\n",
        "            u = int(parts[0])\n",
        "            items = [int(x) for x in parts[1:]]\n",
        "            if not items:\n",
        "                continue\n",
        "            if u not in user_pos_items:\n",
        "                user_pos_items[u] = set()\n",
        "            for i in items:\n",
        "                user_pos_items[u].add(i)\n",
        "                user_ids.append(u)\n",
        "                item_ids.append(i)\n",
        "\n",
        "    user_ids = np.array(user_ids, dtype=np.int64)\n",
        "    item_ids = np.array(item_ids, dtype=np.int64)\n",
        "\n",
        "    num_users = max(user_pos_items.keys()) + 1\n",
        "    num_items = item_ids.max() + 1\n",
        "\n",
        "    return num_users, num_items, user_ids, item_ids, user_pos_items\n"
      ],
      "metadata": {
        "id": "sWet66Lgag2E"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_users, num_items, user_ids_all, item_ids_all, user_pos_items_all = \\\n",
        "    load_amazon_book_train(\"/content/data/amazon-book\")"
      ],
      "metadata": {
        "id": "miAmLYrOew0Y"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_subset(user_pos_items_all, max_users=5000, min_interactions=5):\n",
        "    \"\"\"\n",
        "    Wybiera najaktywniejszych użytkowników (Top-N activity).\n",
        "    Dzięki temu zbiór będzie miał ok. 900k interakcji, podobnie jak w NGCF.\n",
        "    \"\"\"\n",
        "    # 1. Filtr: bierzemy tylko użytkowników, którzy mają minimum interakcji\n",
        "    candidates = [u for u, items in user_pos_items_all.items()\n",
        "                  if len(items) >= min_interactions]\n",
        "\n",
        "    # 2. POPRAWKA: Sortujemy kandydatów według liczby interakcji MALEJĄCO.\n",
        "    #    Wcześniej było: sorted(candidates)[:max_users] (co brało losowych/pierwszych po ID)\n",
        "    candidates = sorted(candidates, key=lambda u: len(user_pos_items_all[u]), reverse=True)[:max_users]\n",
        "\n",
        "    # 3. Tworzymy podzbiór interakcji dla wybranych kandydatów\n",
        "    subset_user_pos_old = {u: set(user_pos_items_all[u]) for u in candidates}\n",
        "\n",
        "    # 4. Zbieramy itemy, które faktycznie występują w tym subsecie (żeby nie trzymać pustych ID)\n",
        "    item_set = set()\n",
        "    for items in subset_user_pos_old.values():\n",
        "        item_set.update(items)\n",
        "\n",
        "    # 5. Mapowanie starych ID na nowe, zwarte (0..N-1)\n",
        "    #    Sortujemy klucze, aby mapowanie było deterministyczne\n",
        "    user_old2new = {u_old: u_new for u_new, u_old in enumerate(sorted(subset_user_pos_old.keys()))}\n",
        "    item_old2new = {i_old: i_new for i_new, i_old in enumerate(sorted(item_set))}\n",
        "\n",
        "    # 6. Nowy słownik z przemapowanymi ID\n",
        "    user_pos_items_sub = {}\n",
        "    for u_old, items_old in subset_user_pos_old.items():\n",
        "        u_new = user_old2new[u_old]\n",
        "        user_pos_items_sub[u_new] = {item_old2new[i_old] for i_old in items_old}\n",
        "\n",
        "    # 7. Tworzenie tablic numpy (potrzebne do budowy macierzy sąsiedztwa)\n",
        "    user_ids_sub = []\n",
        "    item_ids_sub = []\n",
        "    for u_new, items_new in user_pos_items_sub.items():\n",
        "        for i_new in items_new:\n",
        "            user_ids_sub.append(u_new)\n",
        "            item_ids_sub.append(i_new)\n",
        "\n",
        "    user_ids_sub = np.array(user_ids_sub, dtype=np.int64)\n",
        "    item_ids_sub = np.array(item_ids_sub, dtype=np.int64)\n",
        "\n",
        "    num_users_sub = len(user_old2new)\n",
        "    num_items_sub = len(item_old2new)\n",
        "\n",
        "    print(f\"Utworzono podzbiór: Users={num_users_sub}, Items={num_items_sub}, Interactions={len(user_ids_sub)}\")\n",
        "    return num_users_sub, num_items_sub, user_ids_sub, item_ids_sub, user_pos_items_sub"
      ],
      "metadata": {
        "id": "LEosP-LB0OUJ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_users, num_items, user_ids_sub, item_ids_sub, user_pos_items_sub = \\\n",
        "    build_subset(user_pos_items_all, max_users=5000, min_interactions=5)"
      ],
      "metadata": {
        "id": "3opP4bTU0mX4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a245edc-f7c7-4899-90cd-efda69d62904"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Utworzono podzbiór: Users=5000, Items=87590, Interactions=903370\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_split_stats(train_dict, val_dict, test_dict):\n",
        "    def count_interactions(d):\n",
        "        return sum(len(items) for items in d.values())\n",
        "\n",
        "    n_train = count_interactions(train_dict)\n",
        "    n_val   = count_interactions(val_dict)\n",
        "    n_test  = count_interactions(test_dict)\n",
        "\n",
        "    total = n_train + n_val + n_test\n",
        "    if total == 0:\n",
        "        print(\"Brak interakcji, nie ma czego liczyć.\")\n",
        "        return\n",
        "\n",
        "    print(\"Liczba interakcji:\")\n",
        "    print(f\"  train: {n_train}\")\n",
        "    print(f\"  valid: {n_val}\")\n",
        "    print(f\"  test : {n_test}\")\n",
        "    print(f\"  razem: {total}\")\n",
        "\n",
        "    print(\"\\nUdziały procentowe (liczone po liczbie interakcji):\")\n",
        "    print(f\"  train: {100.0 * n_train / total:.2f}%\")\n",
        "    print(f\"  valid: {100.0 * n_val   / total:.2f}%\")\n",
        "    print(f\"  test : {100.0 * n_test  / total:.2f}%\")"
      ],
      "metadata": {
        "id": "GI03uQ7q8Sp6"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_user_interactions(user_pos_items):\n",
        "    train = {}\n",
        "    val = {}\n",
        "    test = {}\n",
        "\n",
        "    for u, items in user_pos_items.items():\n",
        "        items = list(items)\n",
        "        n = len(items)\n",
        "\n",
        "        # za mało interakcji, żeby sensownie dzielić → wszystko do train\n",
        "        if n < 3:\n",
        "            train[u] = set(items)\n",
        "            continue\n",
        "\n",
        "        np.random.shuffle(items)\n",
        "\n",
        "        # docelowo ~10% val, ~10% test, reszta train\n",
        "        n_val = max(1, int(np.floor(0.1 * n)))\n",
        "        n_test = max(1, int(np.floor(0.1 * n)))\n",
        "\n",
        "        # upewnij się, że zostaje przynajmniej 1 próbka w train\n",
        "        if n_val + n_test >= n:\n",
        "            n_val = 1\n",
        "            n_test = 1\n",
        "\n",
        "        n_train = n - n_val - n_test\n",
        "\n",
        "        train_items = items[:n_train]\n",
        "        val_items = items[n_train:n_train + n_val]\n",
        "        test_items = items[n_train + n_val:]\n",
        "\n",
        "        train[u] = set(train_items)\n",
        "        val[u] = set(val_items)\n",
        "        test[u] = set(test_items)\n",
        "\n",
        "    return train, val, test\n"
      ],
      "metadata": {
        "id": "FP9j4CWqdXMa"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_user_pos_items, val_user_pos_items, test_user_pos_items = \\\n",
        "    split_user_interactions(user_pos_items_sub)"
      ],
      "metadata": {
        "id": "IRen_f9seYJo"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compute_split_stats(\n",
        "    train_user_pos_items,\n",
        "    val_user_pos_items,\n",
        "    test_user_pos_items\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j3YC9sCs8WVX",
        "outputId": "fc0e9788-85d0-4f3a-fefc-47ea6c65c124"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Liczba interakcji:\n",
            "  train: 727062\n",
            "  valid: 88154\n",
            "  test : 88154\n",
            "  razem: 903370\n",
            "\n",
            "Udziały procentowe (liczone po liczbie interakcji):\n",
            "  train: 80.48%\n",
            "  valid: 9.76%\n",
            "  test : 9.76%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "BOokBcDlWD0n"
      },
      "outputs": [],
      "source": [
        "def build_normalized_adj(num_users, num_items, user_ids, item_ids):\n",
        "    \"\"\"\n",
        "    num_users: int\n",
        "    num_items: int\n",
        "    user_ids, item_ids: 1D np.array z indeksami (dopasowanymi do [0..))\n",
        "    Zwraca: tf.sparse.SparseTensor reprezentujący Â = D^-1/2 A D^-1/2\n",
        "    \"\"\"\n",
        "    num_nodes = num_users + num_items\n",
        "\n",
        "    # Krawędzie dwudzielne: (u, v) oraz (v, u)\n",
        "    # v = num_users + item\n",
        "    row_idx = []\n",
        "    col_idx = []\n",
        "\n",
        "    for u, i in zip(user_ids, item_ids):\n",
        "        v = num_users + i\n",
        "        row_idx.append(u)\n",
        "        col_idx.append(v)\n",
        "        row_idx.append(v)\n",
        "        col_idx.append(u)\n",
        "\n",
        "    row_idx = np.array(row_idx, dtype=np.int64)\n",
        "    col_idx = np.array(col_idx, dtype=np.int64)\n",
        "\n",
        "    # Stopnie\n",
        "    degrees = np.zeros(num_nodes, dtype=np.float32)\n",
        "    for r, c in zip(row_idx, col_idx):\n",
        "        degrees[r] += 1.0\n",
        "        degrees[c] += 1.0\n",
        "\n",
        "    # Wagi z normalizacją symetryczną\n",
        "    values = []\n",
        "    for r, c in zip(row_idx, col_idx):\n",
        "        deg_r = degrees[r]\n",
        "        deg_c = degrees[c]\n",
        "        if deg_r > 0 and deg_c > 0:\n",
        "            val = 1.0 / np.sqrt(deg_r * deg_c)\n",
        "        else:\n",
        "            val = 0.0\n",
        "        values.append(val)\n",
        "\n",
        "    values = np.array(values, dtype=np.float32)\n",
        "\n",
        "    indices = np.stack([row_idx, col_idx], axis=1)\n",
        "\n",
        "    adj = tf.sparse.SparseTensor(\n",
        "        indices=indices,\n",
        "        values=values,\n",
        "        dense_shape=(num_nodes, num_nodes),\n",
        "    )\n",
        "    # Porządkowanie indeksów\n",
        "    adj = tf.sparse.reorder(adj)\n",
        "    return adj\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LightGCN(tf.keras.Model):\n",
        "    def __init__(self, num_users, num_items, embedding_dim, num_layers, adj):\n",
        "        super().__init__()  # bez tego Keras nic nie śledzi\n",
        "\n",
        "        self.num_users = num_users\n",
        "        self.num_items = num_items\n",
        "        self.num_nodes = num_users + num_items\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # zakładam, że adj to tf.sparse.SparseTensor\n",
        "        self.adj = tf.sparse.reorder(adj)\n",
        "\n",
        "        # JEDYNY trenowalny parametr\n",
        "        self.node_embeddings = self.add_weight(\n",
        "            name=\"node_embeddings\",\n",
        "            shape=(self.num_nodes, embedding_dim),\n",
        "            initializer=tf.keras.initializers.GlorotUniform(),\n",
        "            trainable=True,\n",
        "        )\n",
        "\n",
        "    def propagate(self):\n",
        "        # E^(0)\n",
        "        E_k = self.node_embeddings          # (num_nodes, d)\n",
        "\n",
        "        # od razu zaczynamy sumowanie po warstwach\n",
        "        E_sum = E_k                         # E^(0)\n",
        "\n",
        "        for _ in range(self.num_layers):\n",
        "            # E^{k+1} = Â E^{k}\n",
        "            E_k = tf.sparse.sparse_dense_matmul(self.adj, E_k)\n",
        "            E_sum = E_sum + E_k             # sumujemy E^(k+1)\n",
        "\n",
        "        # średnia po (K+1) poziomach:  (E^0 + E^1 + ... + E^K) / (K+1)\n",
        "        E_final = E_sum / float(self.num_layers + 1)\n",
        "\n",
        "        user_embs = E_final[:self.num_users]\n",
        "        item_embs = E_final[self.num_users:]\n",
        "        return user_embs, item_embs\n"
      ],
      "metadata": {
        "id": "2rcB6bqEaTuZ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bpr_loss(model, user_ids, pos_item_ids, neg_item_ids, l2_reg=1e-4):\n",
        "    user_embs_final, item_embs_final = model.propagate()\n",
        "\n",
        "    u_final = tf.nn.embedding_lookup(user_embs_final, user_ids)\n",
        "    pos_final = tf.nn.embedding_lookup(item_embs_final, pos_item_ids)\n",
        "    neg_final = tf.nn.embedding_lookup(item_embs_final, neg_item_ids)\n",
        "\n",
        "    pos_scores = tf.reduce_sum(u_final * pos_final, axis=-1)\n",
        "    neg_scores = tf.reduce_sum(u_final * neg_final, axis=-1)\n",
        "\n",
        "    diff = pos_scores - neg_scores\n",
        "    loss_per_sample = -tf.math.log_sigmoid(diff)\n",
        "    bpr = tf.reduce_mean(loss_per_sample)\n",
        "\n",
        "    u_ego = tf.nn.embedding_lookup(model.node_embeddings, user_ids)\n",
        "    pos_ego = tf.nn.embedding_lookup(model.node_embeddings, model.num_users + pos_item_ids)\n",
        "    neg_ego = tf.nn.embedding_lookup(model.node_embeddings, model.num_users + neg_item_ids)\n",
        "\n",
        "    reg = tf.nn.l2_loss(u_ego) + tf.nn.l2_loss(pos_ego) + tf.nn.l2_loss(neg_ego)\n",
        "\n",
        "    reg = reg / float(tf.shape(user_ids)[0])\n",
        "\n",
        "    return bpr + l2_reg * reg"
      ],
      "metadata": {
        "id": "12vlf34yaUxu"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_topk(model, user_pos_train, user_pos_eval, num_items, K=20):\n",
        "    user_embs, item_embs = model.propagate()\n",
        "    user_embs = user_embs.numpy()\n",
        "    item_embs = item_embs.numpy()\n",
        "\n",
        "    recalls = []\n",
        "    ndcgs = []\n",
        "\n",
        "    all_items = np.arange(num_items, dtype=np.int64)\n",
        "\n",
        "    for u, eval_items in user_pos_eval.items():\n",
        "        if len(eval_items) == 0:\n",
        "            continue\n",
        "\n",
        "        # scoring wszystkich itemów\n",
        "        u_vec = user_embs[u]  # (d,)\n",
        "        scores = item_embs @ u_vec  # (num_items,)\n",
        "\n",
        "        # maskowanie itemów treningowych (żeby ich nie rekomendować)\n",
        "        train_items = user_pos_train.get(u, set())\n",
        "        exclude = train_items  # opcjonalnie też eval/test, jeśli chcesz czysty ranking\n",
        "        if exclude:\n",
        "            scores[list(exclude)] = -1e9\n",
        "\n",
        "        # top-K\n",
        "        topk_idx = np.argpartition(-scores, K)[:K]\n",
        "        topk_idx = topk_idx[np.argsort(-scores[topk_idx])]\n",
        "\n",
        "        eval_items_list = list(eval_items)\n",
        "\n",
        "        # Recall@K\n",
        "        hit_count = sum(1 for i in eval_items_list if i in topk_idx)\n",
        "        recall = hit_count / len(eval_items_list)\n",
        "        recalls.append(recall)\n",
        "\n",
        "        # NDCG@K\n",
        "        dcg = 0.0\n",
        "        idcg = 0.0\n",
        "        for rank, item in enumerate(topk_idx):\n",
        "            if item in eval_items_list:\n",
        "                dcg += 1.0 / np.log2(rank + 2.0)\n",
        "        # idealne DCG: wszystkie relewantne na początku\n",
        "        for rank in range(min(len(eval_items_list), K)):\n",
        "            idcg += 1.0 / np.log2(rank + 2.0)\n",
        "        ndcg = dcg / idcg if idcg > 0 else 0.0\n",
        "        ndcgs.append(ndcg)\n",
        "\n",
        "    mean_recall = float(np.mean(recalls)) if recalls else 0.0\n",
        "    mean_ndcg = float(np.mean(ndcgs)) if ndcgs else 0.0\n",
        "    return mean_recall, mean_ndcg\n"
      ],
      "metadata": {
        "id": "21AnOqr2ddw9"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dict_to_arrays(user_pos_dict):\n",
        "    user_ids = []\n",
        "    item_ids = []\n",
        "    for u, items in user_pos_dict.items():\n",
        "        for i in items:\n",
        "            user_ids.append(u)\n",
        "            item_ids.append(i)\n",
        "    return np.array(user_ids, dtype=np.int64), np.array(item_ids, dtype=np.int64)"
      ],
      "metadata": {
        "id": "NP5goZ64edvr"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_bpr_sampler(user_pos_items, num_items, batch_size):\n",
        "    \"\"\"\n",
        "    Zwraca generator batchy (user, pos_item, neg_item).\n",
        "    user_pos_items: dict u -> set(itemów pozytywnych)\n",
        "    num_items: liczba itemów\n",
        "    \"\"\"\n",
        "    users = np.array(list(user_pos_items.keys()), dtype=np.int64)\n",
        "    all_items = np.arange(num_items, dtype=np.int64)\n",
        "\n",
        "    def sample_batch():\n",
        "        batch_users = []\n",
        "        batch_pos = []\n",
        "        batch_neg = []\n",
        "\n",
        "        for _ in range(batch_size):\n",
        "            u = int(random.choice(users))\n",
        "            pos_list = list(user_pos_items[u])\n",
        "            i_pos = random.choice(pos_list)\n",
        "\n",
        "            # negatywny: item nie w pozytywnych dla u\n",
        "            while True:\n",
        "                j = int(random.randint(0, num_items - 1))\n",
        "                if j not in user_pos_items[u]:\n",
        "                    i_neg = j\n",
        "                    break\n",
        "\n",
        "            batch_users.append(u)\n",
        "            batch_pos.append(i_pos)\n",
        "            batch_neg.append(i_neg)\n",
        "\n",
        "        return (\n",
        "            np.array(batch_users, dtype=np.int64),\n",
        "            np.array(batch_pos, dtype=np.int64),\n",
        "            np.array(batch_neg, dtype=np.int64),\n",
        "        )\n",
        "\n",
        "    return sample_batch\n"
      ],
      "metadata": {
        "id": "KI1uCQ9Ea5eV"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def train_step(model, user_ids, pos_item_ids, neg_item_ids):\n",
        "    with tf.GradientTape() as tape:\n",
        "        loss = bpr_loss(model, user_ids, pos_item_ids, neg_item_ids)\n",
        "    grads = tape.gradient(loss, model.trainable_variables)\n",
        "\n",
        "    grads_and_vars = [\n",
        "        (g, v)\n",
        "        for g, v in zip(grads, model.trainable_variables)\n",
        "        if g is not None\n",
        "    ]\n",
        "\n",
        "    optimizer.apply_gradients(grads_and_vars)\n",
        "    return loss"
      ],
      "metadata": {
        "id": "9wAjoZz1aXRy"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sampler = make_bpr_sampler(\n",
        "    user_pos_items=train_user_pos_items,\n",
        "    num_items=num_items,\n",
        "    batch_size=2048,  # Amazon-book\n",
        ")"
      ],
      "metadata": {
        "id": "ibfB1N4Pfnhx"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_user_ids, train_item_ids = dict_to_arrays(train_user_pos_items)"
      ],
      "metadata": {
        "id": "ybFLH7zNfnvG"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "adj = build_normalized_adj(\n",
        "    num_users=num_users,\n",
        "    num_items=num_items,\n",
        "    user_ids=train_user_ids,\n",
        "    item_ids=train_item_ids,\n",
        ")"
      ],
      "metadata": {
        "id": "nngAbSOjfru-"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LightGCN(\n",
        "    num_users=num_users,\n",
        "    num_items=num_items,\n",
        "    embedding_dim=64,\n",
        "    num_layers=3,\n",
        "    adj=adj,\n",
        ")\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)"
      ],
      "metadata": {
        "id": "Bj7ax4lkfsa0"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Trainable variables:\")\n",
        "for v in model.trainable_variables:\n",
        "    print(\"  \", v.name, v.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sIlzxJrpp4c4",
        "outputId": "7447ec56-747f-4587-a802-b28ca67e6da9"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trainable variables:\n",
            "   node_embeddings (92590, 64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epoch_history = []\n",
        "loss_history = []\n",
        "val_recall_history = []\n",
        "val_ndcg_history = []"
      ],
      "metadata": {
        "id": "avuTI4eChpZ1"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 2048\n",
        "\n",
        "# 2. Obliczamy całkowitą liczbę interakcji w zbiorze treningowym\n",
        "# train_user_pos_items to słownik {user: set(items)}, więc sumujemy długości zbiorów\n",
        "n_train_interactions = sum(len(items) for items in train_user_pos_items.values())\n",
        "\n",
        "# 3. Wyliczamy ile kroków potrzeba, aby przejść cały zbiór raz (jedna epoka)\n",
        "# Używamy sufitu (ceil), aby uwzględnić ostatni, niepełny batch\n",
        "steps_per_epoch = math.ceil(n_train_interactions / BATCH_SIZE)\n",
        "\n",
        "print(f\"Liczba interakcji: {n_train_interactions}\")\n",
        "print(f\"Batch size: {BATCH_SIZE}\")\n",
        "print(f\"Wyliczone steps_per_epoch: {steps_per_epoch}\")\n",
        "\n",
        "# 4. Konfiguracja treningu\n",
        "num_epochs = 50          # 50 epok jest wystarczające przy poprawnym steps_per_epoch\n",
        "validate_every = 1       # WAŻNE: Zmień na 1. Walidacja co 5 epok przy LightGCN to zbyt rzadko.\n",
        "patience = 10            # Zwiększ patience, skoro sprawdzamy częściej (Early Stopping)\n",
        "\n",
        "# Inicjalizacja zmiennych (bez zmian)\n",
        "best_val_recall = 0.0\n",
        "epochs_no_improve = 0\n",
        "\n",
        "epoch_history = []\n",
        "loss_history = []\n",
        "val_recall_history = []\n",
        "val_ndcg_history = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    epoch_losses = []\n",
        "\n",
        "    for step in range(steps_per_epoch):\n",
        "        u, i_pos, i_neg = sampler()\n",
        "        u = tf.convert_to_tensor(u, dtype=tf.int32)\n",
        "        i_pos = tf.convert_to_tensor(i_pos, dtype=tf.int32)\n",
        "        i_neg = tf.convert_to_tensor(i_neg, dtype=tf.int32)\n",
        "\n",
        "        loss_value = train_step(model, u, i_pos, i_neg)\n",
        "\n",
        "        epoch_losses.append(loss_value.numpy())\n",
        "\n",
        "        # logowanie co jakiś tam krok w epokach\n",
        "        if (step + 1) % 100 == 0:\n",
        "            print(\n",
        "                f\"Epoka {epoch:4d} | Krok {step+1:4d}/{steps_per_epoch} | \"\n",
        "                f\"Loss (ostatni batch): {loss_value.numpy():.4f}\"\n",
        "            )\n",
        "\n",
        "    mean_epoch_loss = float(np.mean(epoch_losses))\n",
        "\n",
        "    # log: średni loss po epoce\n",
        "    print(f\"[Epoka {epoch:4d}] Średni loss: {mean_epoch_loss:.4f}\")\n",
        "\n",
        "    # walidacja co `validate_every` epok\n",
        "    if epoch % validate_every == 0:\n",
        "        val_recall, val_ndcg = evaluate_topk(\n",
        "            model,\n",
        "            user_pos_train=train_user_pos_items,\n",
        "            user_pos_eval=val_user_pos_items,\n",
        "            num_items=num_items,\n",
        "            K=20,\n",
        "        )\n",
        "\n",
        "        print(\n",
        "            f\"[Epoka {epoch:4d}] \"\n",
        "            f\"Val Recall@20: {val_recall:.4f} | Val NDCG@20: {val_ndcg:.4f}\"\n",
        "        )\n",
        "\n",
        "        # zapisz do historii\n",
        "        epoch_history.append(epoch)\n",
        "        loss_history.append(mean_epoch_loss)\n",
        "        val_recall_history.append(val_recall)\n",
        "        val_ndcg_history.append(val_ndcg)\n",
        "\n",
        "        # early stopping po Recall@20\n",
        "        if val_recall > best_val_recall:\n",
        "            best_val_recall = val_recall\n",
        "            epochs_no_improve = 0\n",
        "            print(f\"[Epoka {epoch:4d}] Nowy najlepszy Recall@20: {best_val_recall:.4f}\")\n",
        "            # tutaj ewentualnie snapshot modelu\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "            print(\n",
        "                f\"[Epoka {epoch:4d}] Brak poprawy, patience = \"\n",
        "                f\"{epochs_no_improve}/{patience}\"\n",
        "            )\n",
        "\n",
        "        if epochs_no_improve >= patience:\n",
        "            print(\n",
        "                f\"Early stopping: brak poprawy przez {patience} walidacji. \"\n",
        "                f\"Najlepszy Recall@20 = {best_val_recall:.4f}\"\n",
        "            )\n",
        "            break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "id": "2n1jIz0Zfykj",
        "outputId": "fe811c2f-5048-47df-f44a-c897c3bdd08b"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Liczba interakcji: 727062\n",
            "Batch size: 2048\n",
            "Wyliczone steps_per_epoch: 356\n",
            "Epoka    0 | Krok  100/356 | Loss (ostatni batch): 0.6931\n",
            "Epoka    0 | Krok  200/356 | Loss (ostatni batch): 0.6931\n",
            "Epoka    0 | Krok  300/356 | Loss (ostatni batch): 0.6920\n",
            "[Epoka    0] Średni loss: 0.6925\n",
            "[Epoka    0] Val Recall@20: 0.0151 | Val NDCG@20: 0.0165\n",
            "[Epoka    0] Nowy najlepszy Recall@20: 0.0151\n",
            "Epoka    1 | Krok  100/356 | Loss (ostatni batch): 0.6507\n",
            "Epoka    1 | Krok  200/356 | Loss (ostatni batch): 0.5781\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-132616086.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi_pos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi_neg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0mu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mi_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi_pos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3522465830.py\u001b[0m in \u001b[0;36msample_batch\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0musers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mpos_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_pos_items\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0mi_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0;31m# negatywny: item nie w pozytywnych dla u\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/random.py\u001b[0m in \u001b[0;36mchoice\u001b[0;34m(self, seq)\u001b[0m\n\u001b[1;32m    346\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Cannot choose from an empty sequence'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 348\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_randbelow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/random.py\u001b[0m in \u001b[0;36m_randbelow_with_getrandbits\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    245\u001b[0m         \u001b[0mgetrandbits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetrandbits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbit_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetrandbits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 0 <= r < 2**k\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mr\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetrandbits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# loss\n",
        "plt.figure()\n",
        "plt.plot(epoch_history, loss_history)\n",
        "plt.xlabel(\"Epoka\")\n",
        "plt.ylabel(\"Loss (średni)\")\n",
        "plt.title(\"Historia lossu\")"
      ],
      "metadata": {
        "id": "G4ezCjwahtSZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Recall@K\n",
        "plt.figure()\n",
        "plt.plot(epoch_history, val_recall_history)\n",
        "plt.xlabel(\"Epoka\")\n",
        "plt.ylabel(\"Recall@20 (wal)\")\n",
        "plt.title(\"Recall@20 na walidacji\")"
      ],
      "metadata": {
        "id": "R0NZRzmDqHaE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NDCG@K\n",
        "plt.figure()\n",
        "plt.plot(epoch_history, val_ndcg_history)\n",
        "plt.xlabel(\"Epoka\")\n",
        "plt.ylabel(\"NDCG@20 (wal)\")\n",
        "plt.title(\"NDCG@20 na walidacji\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "I9oWckf5qIyd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "K = 20  # albo inna wartość, jeśli chcesz\n",
        "\n",
        "test_recall, test_ndcg = evaluate_topk(\n",
        "    model,\n",
        "    user_pos_train=train_user_pos_items,\n",
        "    user_pos_eval=test_user_pos_items,\n",
        "    num_items=num_items,\n",
        "    K=K,\n",
        ")\n",
        "\n",
        "print(f\"Test Recall@{K}: {test_recall:.4f}\")\n",
        "print(f\"Test NDCG@{K}:  {test_ndcg:.4f}\")\n"
      ],
      "metadata": {
        "id": "wgYnQ7puA5fJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}